{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49389227-6ea2-4796-9c07-7459dc325fee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ctypes import util\n",
    "from cv2 import IMREAD_GRAYSCALE\n",
    "import torch\n",
    "import utils as utils\n",
    "import torch.utils.data.dataset as Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import lmdb\n",
    "import io\n",
    "import time\n",
    "from vidaug import augmentors as va\n",
    "from augmentation import *\n",
    "import yaml\n",
    "from loguru import logger\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68ed57f3-f96f-4b5c-93c5-901b50aa45d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./configs/config_gloss_free.yaml', 'r+',encoding='utf-8') as f:\n",
    "    config = yaml.load(f,Loader=yaml.FullLoader)\n",
    "    \n",
    "parser = argparse.ArgumentParser('Gloss-free Sign Language Translation script', add_help=False)\n",
    "parser.add_argument('--batch-size', default=1, type=int)\n",
    "parser.add_argument('--epochs', default=1, type=int)\n",
    "parser.add_argument('--lr', type=float, default=1.0e-3, metavar='LR',\n",
    "                help='learning rate (default: 5e-4)')\n",
    "parser.add_argument('--seed', default=0, type=int)\n",
    "parser.add_argument('--output_dir', default='./output/slt',\n",
    "                help='path where to save, empty for no saving')\n",
    "parser.add_argument('--device', default='cuda',\n",
    "                help='device to use for training / testing')\n",
    "parser.add_argument('--num_workers', default=0, type=int)\n",
    "parser.add_argument('--config', type=str, default='./configs/config_gloss_free.yaml')\n",
    "\n",
    "\n",
    "# * distributed training parameters\n",
    "parser.add_argument('--world_size', default=1, type=int,\n",
    "                help='number of distributed processes')\n",
    "parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "parser.add_argument('--local_rank', default=0, type=int)\n",
    "\n",
    "# * Finetuning params\n",
    "parser.add_argument('--finetune', default='', help='finetune from checkpoint')\n",
    "\n",
    "# * Optimizer parameters\n",
    "parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                help='Optimizer (default: \"adamw\"')\n",
    "parser.add_argument('--opt-eps', default=1.0e-09, type=float, metavar='EPSILON',\n",
    "                help='Optimizer Epsilon (default: 1.0e-09)')\n",
    "parser.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                help='Optimizer Betas (default: None, use opt default)')\n",
    "parser.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n",
    "                help='Clip gradient norm (default: None, no clipping)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.001,\n",
    "                help='weight decay (default: 0.05)')\n",
    "\n",
    "# * Learning rate schedule parameters\n",
    "parser.add_argument('--sched', default='cosine', type=str, metavar='SCHEDULER',\n",
    "                help='LR scheduler (default: \"cosine\"')\n",
    "\n",
    "parser.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n",
    "                help='learning rate noise on/off epoch percentages')\n",
    "parser.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n",
    "                help='learning rate noise limit percent (default: 0.67)')\n",
    "parser.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n",
    "                help='learning rate noise std-dev (default: 1.0)')\n",
    "parser.add_argument('--warmup-lr', type=float, default=1e-6, metavar='LR',\n",
    "                help='warmup learning rate (default: 1e-6)')\n",
    "parser.add_argument('--min-lr', type=float, default=1.0e-08, metavar='LR',\n",
    "                help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n",
    "\n",
    "parser.add_argument('--decay-epochs', type=float, default=30, metavar='N',\n",
    "                help='epoch interval to decay LR')\n",
    "parser.add_argument('--warmup-epochs', type=int, default=0, metavar='N',\n",
    "                help='epochs to warmup LR, if scheduler supports')\n",
    "parser.add_argument('--cooldown-epochs', type=int, default=10, metavar='N',\n",
    "                help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\n",
    "parser.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n",
    "                help='patience epochs for Plateau LR scheduler (default: 10')\n",
    "parser.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n",
    "                help='LR decay rate (default: 0.1)')\n",
    "parser.add_argument('--decoder_type', default='LD')\n",
    "parser.add_argument('--noise_rate', type=float, default=0.15)\n",
    "parser.add_argument('--noise_type', default='omit_last')\n",
    "parser.add_argument('--random_shuffle', default=False)\n",
    "\n",
    "# * Baise params\n",
    "\n",
    "\n",
    "parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                help='start epoch')\n",
    "parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n",
    "parser.add_argument('--dist-eval', action='store_true', default=False, help='Enabling distributed evaluation')\n",
    "parser.add_argument('--pin-mem', action='store_true',\n",
    "                help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--no-pin-mem', action='store_false', dest='pin_mem',\n",
    "                help='')\n",
    "parser.set_defaults(pin_mem=True)\n",
    "\n",
    "\n",
    "# *Drop out params\n",
    "parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                help='Dropout rate (default: 0.)')\n",
    "parser.add_argument('--drop-path', type=float, default=0.1, metavar='PCT',\n",
    "                help='Drop path rate (default: 0.1)')\n",
    "\n",
    "# * Mixup params\n",
    "parser.add_argument('--mixup', type=float, default=0.0,\n",
    "                help='mixup alpha, mixup enabled if > 0. (default: 0.8)')\n",
    "parser.add_argument('--cutmix', type=float, default=0.0,\n",
    "                help='cutmix alpha, cutmix enabled if > 0. (default: 1.0)')\n",
    "parser.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n",
    "                help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "parser.add_argument('--mixup-prob', type=float, default=1.0,\n",
    "                help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "parser.add_argument('--mixup-switch-prob', type=float, default=0.5,\n",
    "                help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "parser.add_argument('--mixup-mode', type=str, default='batch',\n",
    "                help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "# * data process params\n",
    "parser.add_argument('--input-size', default=224, type=int)\n",
    "parser.add_argument('--resize', default=256, type=int)\n",
    "# * visualization\n",
    "parser.add_argument('--visualize', action='store_true')\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d471e4a-9080-447e-a7dc-bc55e871bb40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class S2T_Dataset(Dataset.Dataset):\n",
    "    def __init__(self, path, phase, args, config, seed=None, training_refurbish=False, aug_rate=0.5):\n",
    "        # 生成随机种子\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        self.max_length = config['data']['max_length']\n",
    "        # self.max_length = 10\n",
    "\n",
    "        self.img_path = config['data']['img_path']\n",
    "        self.kps_path = config['data']['keypoint_path']\n",
    "\n",
    "        self.args = args\n",
    "        self.aug_rate = aug_rate\n",
    "        self.phase = phase\n",
    "        self.config = config\n",
    "\n",
    "        self.raw_data = utils.load_dataset_file(path)\n",
    "        self.training_refurbish = training_refurbish\n",
    "        self.list = [key for key, value in self.raw_data.items()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.list[index]\n",
    "        sample = self.raw_data[key]\n",
    "        tgt_sample = sample['text']\n",
    "        length = sample['length']\n",
    "\n",
    "        name_sample = sample['name']\n",
    "\n",
    "        img_sample = self.load_imgs([self.img_path + x for x in sample['imgs_path']], index, include_blur=True)\n",
    "        kp_sample = self.load_imgs([self.kps_path + x for x in sample[\"kps_path\"]], index, include_blur=False)\n",
    "\n",
    "        return name_sample, img_sample, kp_sample, tgt_sample\n",
    "\n",
    "    def load_imgs(self, paths, index, include_blur=True):\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        paths = self.length_constraint(paths)\n",
    "        imgs = torch.zeros(len(paths), 3, self.args.input_size, self.args.input_size)\n",
    "\n",
    "        batch_image = []\n",
    "        crop_rect, resize = self.data_augmentation(resize=(self.args.resize, self.args.resize),\n",
    "                                                   crop_size=self.args.input_size,\n",
    "                                                   is_train=(self.phase == 'train'), index=index)\n",
    "        # print('img', crop_rect, resize)\n",
    "        for i, img_path in enumerate(paths):\n",
    "            # print(img_path)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"警告：无法加载位于 {img_path} 的图像。\")\n",
    "                continue  # 跳过这张图像\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(img)\n",
    "            batch_image.append(img)\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            seq = self.video_augmentation(index,include_blur=include_blur)\n",
    "            batch_image = seq(batch_image)\n",
    "            # pass\n",
    "\n",
    "        for i, img in enumerate(batch_image):\n",
    "            img = img.resize(resize)\n",
    "            img = data_transform(img).unsqueeze(0)\n",
    "            imgs[i, :, :, :] = img[:, :, crop_rect[1]:crop_rect[3], crop_rect[0]:crop_rect[2]]\n",
    "            \n",
    "            save_path = paths[i].replace('/frame/', '/aug_frame/')  # 根据实际路径逻辑修改\n",
    "            img_pil = transforms.ToPILImage()(img.squeeze(0))\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)  # 确保目录存在\n",
    "            img_pil.save(save_path)\n",
    "        return imgs\n",
    "\n",
    "    def length_constraint(self, paths):\n",
    "        if len(paths) > self.max_length:\n",
    "            random.seed(self.seed)\n",
    "            tmp = sorted(random.sample(range(len(paths)), k=self.max_length))\n",
    "            new_paths = []\n",
    "            for i in tmp:\n",
    "                new_paths.append(paths[i])\n",
    "            paths = new_paths\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def data_augmentation(self, resize=(320, 240), crop_size=224, is_train=True, index=0):\n",
    "        new_seed = hash((self.seed, index)) % (2 ** 32)\n",
    "        np.random.seed(new_seed)  # 同步设置 NumPy 的随机种子，如果需要\n",
    "        if is_train:\n",
    "            left = np.random.randint(0, resize[0] - crop_size)\n",
    "            top = np.random.randint(0, resize[1] - crop_size)\n",
    "        else:\n",
    "            left = (resize[0] - crop_size) // 2\n",
    "            top = (resize[1] - crop_size) // 2\n",
    "\n",
    "        return (left, top, left + crop_size, top + crop_size), resize\n",
    "\n",
    "    def video_augmentation(self, index=0, include_blur=True):\n",
    "        new_seed = hash((self.seed, index)) % (2 ** 32)\n",
    "        random.seed(new_seed)\n",
    "        sometimes = lambda aug: va.Sometimes(self.aug_rate, aug)\n",
    "\n",
    "        augmentations = [\n",
    "            sometimes(va.RandomRotate(30)),\n",
    "            sometimes(va.RandomResize(0.2)),\n",
    "            sometimes(va.RandomTranslate(x=50, y=50))\n",
    "        ]\n",
    "\n",
    "        if include_blur:\n",
    "            augmentations.append(sometimes(va.GaussianBlur(sigma=2)))  # 只对视频应用模糊\n",
    "            # augmentations.append(sometimes(va.Sharpness(alpha=(0.5, 1.5)))),  # 调整锐度，alpha 是锐度强度的范围\n",
    "            augmentations.append(sometimes(va.Multiply(value=1.0))),  # 类似对比度调整\n",
    "\n",
    "            augmentations.append(sometimes(Brightness(min=0.1, max=1.5))),\n",
    "            # SomeOf(self.seq_geo, self.seq_color)\n",
    "        seq = va.Sequential(augmentations)\n",
    "        return seq\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'#total {self.phase} set: {len(self.list)}.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7cbfaf5-0c75-4d2a-a17e-c3d979a57d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Multiply in module vidaug.augmentors.intensity:\n",
      "\n",
      "class Multiply(builtins.object)\n",
      " |  Multiply(value=1.0)\n",
      " |  \n",
      " |  Multiply all pixel intensities with given value.\n",
      " |  This augmenter can be used to make images lighter or darker.\n",
      " |  \n",
      " |  Args:\n",
      " |      value (float): The value with which to multiply the pixel intensities\n",
      " |      of video.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, clip)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __init__(self, value=1.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(va.Multiply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba718ca-1b17-494e-af3d-49d1e3531596",
   "metadata": {},
   "source": [
    "# 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c05556b-f843-4348-9d58-7a642a74cb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = S2T_Dataset(path=config['data']['train_label_path'], config=config, args=args, phase='train', aug_rate=1,seed = 0)\n",
    "dev_data = S2T_Dataset(path=config['data']['dev_label_path'],  config=config, args=args, phase='train', aug_rate=1, seed = 0)\n",
    "test_data = S2T_Dataset(path=config['data']['test_label_path'],  config=config, args=args, phase='train', aug_rate=1,seed = 0)\n",
    "\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    data = train_data[i]\n",
    "for i in range(len(dev_data)):\n",
    "    data = dev_data[i]\n",
    "for i in range(len(test_data)):\n",
    "    data = test_data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4dee80-a9ae-4d91-b640-87830537f6f6",
   "metadata": {},
   "source": [
    "# 92条数据增强10次 存放在不同位置， 然后用数据集拼接的方法拼接， 然后在随机选取大部分作为训练集，小部分作为验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a083b7b5-c4be-40ab-8e7a-39b0829630bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = S2T_Dataset(path=config['data']['train_label_path'], config=config, args=args, phase='train', aug_rate=1,seed = 1)\n",
    "dev_data = S2T_Dataset(path=config['data']['dev_label_path'],  config=config, args=args, phase='train', aug_rate=1, seed = 1)\n",
    "test_data = S2T_Dataset(path=config['data']['test_label_path'],  config=config, args=args, phase='train', aug_rate=1,seed = 1)\n",
    "\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    data = train_data[i]\n",
    "for i in range(len(dev_data)):\n",
    "    data = dev_data[i]\n",
    "for i in range(len(test_data)):\n",
    "    data = test_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a806302-194a-4a7d-b0e1-c9d782999dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = S2T_Dataset(path=config['data']['train_label_path'], config=config, args=args, phase='train', aug_rate=1,seed = 2)\n",
    "dev_data = S2T_Dataset(path=config['data']['dev_label_path'],  config=config, args=args, phase='train', aug_rate=1, seed = 2)\n",
    "test_data = S2T_Dataset(path=config['data']['test_label_path'],  config=config, args=args, phase='train', aug_rate=1,seed = 2)\n",
    "\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    data = train_data[i]\n",
    "for i in range(len(dev_data)):\n",
    "    data = dev_data[i]\n",
    "for i in range(len(test_data)):\n",
    "    data = test_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6d2e9e0-65e8-4d47-a587-55c7caa621fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = S2T_Dataset(path=config['data']['train_label_path'], config=config, args=args, phase='train', aug_rate=1,seed = 3)\n",
    "dev_data = S2T_Dataset(path=config['data']['dev_label_path'],  config=config, args=args, phase='train', aug_rate=1, seed = 3)\n",
    "test_data = S2T_Dataset(path=config['data']['test_label_path'],  config=config, args=args, phase='train', aug_rate=1,seed = 3)\n",
    "\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    data = train_data[i]\n",
    "for i in range(len(dev_data)):\n",
    "    data = dev_data[i]\n",
    "for i in range(len(test_data)):\n",
    "    data = test_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c55fd1a4-9c07-419c-aec6-041a7a855fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = S2T_Dataset(path=config['data']['train_label_path'], config=config, args=args, phase='train', aug_rate=1,seed = 4)\n",
    "dev_data = S2T_Dataset(path=config['data']['dev_label_path'],  config=config, args=args, phase='train', aug_rate=1, seed = 4)\n",
    "test_data = S2T_Dataset(path=config['data']['test_label_path'],  config=config, args=args, phase='train', aug_rate=1,seed = 4)\n",
    "\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    data = train_data[i]\n",
    "for i in range(len(dev_data)):\n",
    "    data = dev_data[i]\n",
    "for i in range(len(test_data)):\n",
    "    data = test_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfff1c05-183c-49e9-8c23-da74bd85318a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = S2T_Dataset(path=config['data']['train_label_path'], config=config, args=args, phase='train', aug_rate=1,seed = 5)\n",
    "dev_data = S2T_Dataset(path=config['data']['dev_label_path'],  config=config, args=args, phase='train', aug_rate=1, seed = 5)\n",
    "test_data = S2T_Dataset(path=config['data']['test_label_path'],  config=config, args=args, phase='train', aug_rate=1,seed = 5)\n",
    "\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    data = train_data[i]\n",
    "for i in range(len(dev_data)):\n",
    "    data = dev_data[i]\n",
    "for i in range(len(test_data)):\n",
    "    data = test_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79b70e3f-256c-4f40-8215-3da120ee5179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = S2T_Dataset(path=config['data']['train_label_path'], config=config, args=args, phase='train', aug_rate=1,seed = 6)\n",
    "dev_data = S2T_Dataset(path=config['data']['dev_label_path'],  config=config, args=args, phase='train', aug_rate=1, seed = 6)\n",
    "test_data = S2T_Dataset(path=config['data']['test_label_path'],  config=config, args=args, phase='train', aug_rate=1,seed = 6)\n",
    "\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    data = train_data[i]\n",
    "for i in range(len(dev_data)):\n",
    "    data = dev_data[i]\n",
    "for i in range(len(test_data)):\n",
    "    data = test_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c8007-7e7b-43af-bda0-4818eecc3f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
